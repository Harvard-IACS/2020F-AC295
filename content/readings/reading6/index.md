Title: Reading Discussion 6: Distillation and Compression
Category: readings
Date: 2020-01-12
Author: Pavlos Protopapas
Slug: reading6
Tags: Compression, Distillation 

## Selected Readings
#### Expository

#### Research
<!--
- [An embarrassingly simple approach to zero-shot learning](http://proceedings.mlr.press/v37/romera-paredes15.pdf)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- [Distilling a neural network into a soft decision tree](https://arxiv.org/pdf/1711.09784.pdf)
- [Model compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)
-->
#### Resources
<!--
- [NVIDIA Transfer Learning Toolkit](https://developer.nvidia.com/transfer-learning-toolkit)
-->