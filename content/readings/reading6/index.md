Title: Reading Discussion 6
Category: readings
Date: 2020-10-15
Author: Pavlos Protopapas
Slug: reading6
Tags: Transfer Learning


## Selected Readings
- # Expository
	- Sebastian Ruder: [NLP's ImageNet moment has arrived](https://thegradient.pub/nlp-imagenet/).
	- Jay Alammar: [The Illustrated Word2Vec](http://jalammar.github.io/illustrated-word2vec/). Visual introduction to word embeddings, language models, bags of words and skip grams.
    - Chris McCormick's [tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) on the Word2Vec skip-gram model and [article](http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/) on its application to recommenders.
    - [Jurafsky and Martin (2019), 'N-gram Language Models'](https://web.stanford.edu/~jurafsky/slp3/3.pdf), ยง3.1 (Ngrams), ยง3.2 (Evaluating Language Models), and ยง3.3 (Generalization and Zeros). (The other sections are interesting too, but tangential to this course.)

	- Mihail Eric:[ Deep Contextualized Word Representations with ELMo](https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo)

    - FastAI's 'code-first' lectures on [language modelling](https://www.youtube.com/watch?v=PNNHaQUQqW8&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9) and [transfer learning for NLP](https://www.youtube.com/watch?v=5gCQvuznKn0&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9) (2019 course).
    - [Evaluation Metrics for Language Modelling](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/). An introduction to perplexity, bits-per-character, and cross entropy.
    - Sebastian Ruder: [The State of Transfer Learning in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/) (blog post) and [Transfer Learning in Open-Source Natural Language Processing](https://www.youtube.com/watch?v=hNPwRPg9BrQ&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc) (conference presentation).
    - [Tracking Progress in Natural Language Processing](https://nlpprogress.com/).

<br>

- # Use Cases
    - [TensorFlow Embedding Projector](https://projector.tensorflow.org/). An excellent tool to visualise the Word2Vec or your own embeddings in 2-3 dimensions. Projection is done using either [PCA](https://github.com/cs109/2015/blob/master/Lectures/09-ClassificationPCA.pdf), [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), or [UMAP](https://pair-code.github.io/understanding-umap/).
    - [Using Word2vec for Music Recommendations](https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484).

<br> 

- # Research
    - [Mikolov et al (2013a), 'Efficient Estimation of Word Representations in Vector Space'](https://arxiv.org/abs/1301.3781).  
    - [Mikolov et al (2013b), 'Distributed Representations of Words and Phrases and their Compositionality'](https://arxiv.org/abs/1310.4546).  
    - [Pennington et al (2014), 'GLoVe: Global Vectors for Word Representation'](https://nlp.stanford.edu/projects/glove/). 
    - [Y. Kim et al 2015, Character-Aware Neural Language Models](https://arxiv.org/pdf/1508.06615.pdf)
    - [K. W. Zhang and S. Bowman (2019), More than Syntax](https://arxiv.org/abs/1809.10040)
	- [J. Howard and S. Ruder, 2018, Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)
    - [Peters et al (2018), 'Deep contextualized word representations'](https://arxiv.org/abs/1802.05365). 
    - [Yang et al (2019), 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'](https://arxiv.org/abs/1906.08237).     
    - [McCann et al, 2018, Learned in Translation: Contextualized Word Vectors](https://arxiv.org/pdf/1708.00107.pdf)


<br>
* Next presentations, select from Research or Use Case