Title: Reading Discussion 7
Category: readings
Date: 2020-10-23
Author: Pavlos Protopapas
Slug: reading7
Tags: Language Modelling, Attention, Transformers



## Selected Readings

- # Expository

- Adam Kosiorek: [Attention in Neural Networks and How to Use It](http://akosiorek.github.io/ml/2017/10/14/visual-attention.html)  

- Lilian Weng: [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

- Jay Alammar: [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) and [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/) and [Visual guide to using BERT for the first time.](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)

- [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)


- Yannic Kilcher's videos explaining the [transformers](https://www.youtube.com/watch?v=iDulhoQ2pro) and [BERT](https://www.youtube.com/watch?v=-9evrZnBorM) and [GPT-3](https://www.youtube.com/watch?v=SY5PvZrJhLE) papers. 
 
 
- Chris McCormick's [BERT Research Series](https://www.youtube.com/playlist?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6). A YouTube playlist covering word embeddings, attention, positional encodings, masked language models and fine-tuning.

<br>

- # Use Cases
- [spaCy](https://spacy.io/). An excellent library for using language models in production. <br/>
        - [spaCy meets Transformers: Fine-tune BERT, XLNet and GPT-2](https://explosion.ai/blog/spacy-transformers). 

- [spaCy IRL 2019](https://www.youtube.com/playlist?list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc)

<!-- A guide on how to use [Hugging Face](https://huggingface.co/)'s [pre-trained NLP models](https://github.com/huggingface/transformers) in spaCy.
- [spaCy IRL 2019](https://www.youtube.com/playlist?list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc). Conference presentations covering the use of NLP models in industry, including [conversational AI assistants](https://www.youtube.com/watch?v=1jI0mTcNRUU&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc); [scientific and biomedical text](https://www.youtube.com/watch?v=2_HSKDALwuw&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc); [finance](https://www.youtube.com/watch?v=rdmaR4WRYEM&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc); and [asset management](https://www.youtube.com/watch?v=kX14Ycieju8&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc). -->

- [Write With Transformer](https://transformer.huggingface.co/). [Hugging Face](https://huggingface.co)'s interactive demonstration of GPT-2 and XLNET's predictive power. 

- Gwern Branden's [GPT-3 page](https://www.gwern.net/GPT-3). Discussions on how GPT-3 is programmed using prompts; its limitations; examples of poetry and prose generated in the style of famous authors, philosophers, etc.; its performance on logic and arithmetic tasks.
<br>
 
- # Research
- [Vaswani et al (2017), 'Attention is All you Need'](https://arxiv.org/abs/1706.03762). Introduces the Transformer, the neural network architecture used by the most powerful language models. [Sasha Rush](http://rush-nlp.com/) has an excellent [line-by-line PyTorch implementation](http://nlp.seas.harvard.edu/2018/04/03/attention.html) of this paper.

- [Devlin et al (2019), 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'](https://arxiv.org/abs/1810.04805)

- [OpenAI (2020), 'Language Models are Few-Shot Learners' (GPT-3 paper)](https://arxiv.org/pdf/2005.14165.pdf)

- [Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)
- [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf)
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
- [One Model To Learn Them All](https://arxiv.org/pdf/1706.05137.pdf)
- [How to Fine-Tune BERT for Text Classification](https://arxiv.org/pdf/1905.05583.pdf)




    
<br>
* Next presentations, select from Research or Use Case