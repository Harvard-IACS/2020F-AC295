Title: Reading Discussion 8
Category: readings
Date: 2020-10-23
Author: Pavlos Protopapas
Slug: reading8
Tags: Distillation, Compression



## Selected Readings

- # Expository
- [Knowledge Distillation](https://medium.com/neuralmachine/knowledge-distillation-dc241d7c2322)
- [An Overview of Model Compression Techniques for Deep Learning in Space](https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5)
- [Deep Learning - Model Optimization and Compression: Simplified](https://towardsdatascience.com/machine-learning-models-compression-and-quantization-simplified-a302ddf326f2)
- [Distillation of Knowledge in Neural Networks](https://towardsdatascience.com/distillation-of-knowledge-in-neural-networks-cc02f79698b6)

 
- # Research
- [Model compression](http://www.niculescu-mizil.org/papers/rtpp364-bucila.rev2.pdf)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525)
- [An End-to-End Compression Framework Based on Convolutional Neural Networks](https://arxiv.org/pdf/1708.00838v1.pdf)
- [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf)
- [Structured Knowledge Distillation for Semantic Segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.pdf)
- [Improved knowledge distillation via teacher assistant](https://arxiv.org/abs/1902.03393)
- [On the Efficacy of Knowledge Distillation](https://arxiv.org/pdf/1910.01348.pdf)


    
<br>
* Next presentations, select from Research or Use Case