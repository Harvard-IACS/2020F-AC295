var tipuesearch = {"pages":[{"title":"Calendars","text":"","tags":"pages","url":"pages/calendars.html"},{"title":"FAQ","text":"General Do I have access to the video recorded materials? Yes. All AC295 students have access to live-streaming and to all the video captured materials. Is there a GitHub repo for the class? Yes. https://github.com/Harvard-IACS/2020F-AC295 Does the individual Exercise (EX) mean I have to submit on my own but can I still work with my HW partner? You are supposed to work on your own, and you should not work with a partner. You can ask questions at Office Hours (OHs) and use all of your materials from the course up to the EX. I am an Extension School Student, can I participate to the forum discussion? And do they count towards my grade? All AC295 students -both Extension and Non-Extension- have access and should participate to the forum discussion. The grades recevied from this activity will count 10% of your final grade. Can I make-up for the forum discussion grade? The forum discussion is an activity that allow the students to solidify the materials tought in class and expand it with readings. Given the importance of this activity, you will not receive extra exercise to compensate poor grade. Project Topic A significant part of this course is a group project. You will work in small teams on a project about a topic of your choosing from those taught in class. You will acquire the data, design, and implement your application using the tools and techniques learned during the course. Auditors Can I audit this course? This course is based on frequent interactions among teaching staff and students. Given the nature of the class and remote teaching, our policy is not to allow auditors.","tags":"pages","url":"pages/faq.html"},{"title":"Projects","text":"Project Topic A significant part of this course is a group project. You will work in small teams on a project about a topic of your choosing from those taught in class. You will acquire the data, design, and implement your application using the tools and techniques learned during the course. Team You will work closely with other classmates in a 3-4 person project team. Team formation will be discussed in class at the beginning of the course. Milestones The project consists of several milestones that need to be submitted on the specified due dates as listed below. Milestone Assignment Due date 1 Project proposal and detailed Scope of Work (SOW) 10/29/2020 2 Reviewed SOW and Kick Off 11/17/2020 3 Mid Project Review and Testing 12/03/2020 4 Final Projects Presentations 12/11/2020 Deliverables Final Deliverables - presentation (due: 12/11) and medium post (due: 12/15) Previous projects You can find some examples of the projects developed by AC295 students during the previous iterations of this course on Medium in the publication curated by the Institute of Applied Computational Science and on the page dedicated to AC295 Projects . Additional AC295 students' Final Projects can be found on Medium in Towards Data Science , the publication specialized in Data Science. Check these links: Image Segmentation Tool , Web Based Transfer Learning Portal .","tags":"pages","url":"pages/projects.html"},{"title":"Resources","text":"GitHub Repo","tags":"pages","url":"pages/resources.html"},{"title":"Schedule","text":"Week Lecture (Tuesday) Lecture (Thursday) Assignment (R:release - D:due) 1 No Class Lecture 1: Introduction: Virtual Enviroments and Virtual Boxes No assignment 2 Lecture 2: Containers Reading Discussion 1 R:EX1 3 Lecture 3: Kubernetes Reading Discussion 2 R:EX2 - D:EX1 4 Lecture 4: Dask Reading Discussion 3 R:EX3 - D:EX2 5 Practicum 1: End to end art search engine Practicum 1 No assignment 6 Lecture 5: Intro to Transfer Learning: basics and CNNs review Reading Discussion 4 R:EX4 - D:EX3 7 Lecture 6: Transfer Learning for Images and SOTA Models Reading Discussion 5 R:EX5 - D:EX4 8 Lecture 7: Language Models and Transfer Learning for Text Reading Discussion 6 R:EX6 - D:EX5 9 Lecture 8: Attention and Transformers Reading Discussion 7 R:EX7 - D:EX6 10 Lecture 9: Distillation and Compression Reading Discussion 8 R:EX8 - D:EX7 11 Practicum 2 Practicum 2 D:EX8 12 Lecture 10: Introduction and Overview of Viz for Deep Models: lime and shapley Lecture 11: CNNs for Image Data, Activation Maximization and Saliency Maps No assignment 13 Lecture 12: Attention for Debugging Language Models No Class: Thanksgiving No assignment 14 Final Project Final Project No assignment 14 Final Project Final Project Presentation No assignment","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"Course helpline: ac295f2020@gmail.com Welcome to AC295: advanced practical data science. The course will be divided into three major topics: 1. How to scale a model from a prototype (often in jupyter notebooks) to the cloud. In this module, we cover virtual environments, containers, and virtual machines before learning about containers and Kubernetes. Along the way, students will be exposed to Dask. 2. How to use existing models for transfer learning. Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. It is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks. This could be very important, given the vast computing resources required to develop neural network models on these problems and the huge gains that these models can provide. In this part of the course, we will examine various pre-existing models and techniques in transfer learning. 3. In the third part, we will introduce several intuitive visualization tools for investigating and diagnosing models. We will be demonstrating a number of visualization tools ranging from the well established (like saliency maps) to recent examples that have appeared in https://distill.pub. Lectures (online): Tuesday and Thursday 10:30-11:45am (and possibly depending on timezone of students repeat Tuesday and Thursday from 6:00-7:15pm) TFs: Rashmi Banthia, William Palmer, Andrea Porelli, Shivas Jayaram, Hai Bui, Javid Lakha, Yujiao Chen Office Hours: TBD List of Contents Prerequisites Software Topics Course Activities Resources Assignments Getting Help Course Policies Prerequisites Students are expected to be fluent in programming (Python), statistics knowledge at the level of Stat 110 or above, data science (or machine learning) at the level of AC209A and AC209B. Software We will be using a variety of software, primarily Python 3, Pytorch, Tensorflow, and Docker. More details in class. Topics The course is organized in three modules. Deploy data science (integration + scalability) 1a. Virtual Environments and Virtual Boxes 1b. Containers 1c. Kubernetes 1d. Dask Transfer learning and distillation 2a. Intro to Transfer Learning: basics and Convolutional Neural Networks (CNNs) review 2b. Transfer Learning for Images and SOTA Models 2c. Language Models and Transfer Learning with Text Data 2d. Attention and Transformers 2e. Distillation and Compression Visualization as investigative tool 3a. Introduction and Overview of Viz for Deep Models: lime and shapley 3b. CNNs for Image Data, Activation Maximization and Saliency Maps 3c. Attention for Debugging Language Models Course Activities Each module is structured in three types of activities: Lectures , Reading Discussion , and Practicum . Each activity requires the students to complete different assignments in the form of exercise/homework, discussions, reading assignments, and presentations (see Assignments below). During the first weeks of each module, students will attend a lecture on Tuesday and reading discussion on Thursday. The last week of each module will be Practicum. Attendance is mandatory . Reading List consists of papers, blogs, and other reading material that will be released no later than the beginning of each week. This will be the base for all the activities during the week. See Readings Guidelines here link to guidelines . Lectures are held online Tuesdays from 10:30-11:45am (and possibly depending on timezone of students repeat Tuesday from 6:00-7:15pm). During this activity, we will discuss and summarize the basic concepts of the material covered during the week. Journal Discussions are held online on Thursdays from 10:30-11:45 am (and possibly depending on timezone of students repeat Thursday from 6:00-7:15 pm). During this activity, two groups will present one paper each from the Reading List to the rest of the class and lead the discussion. See Paper Presentation Guidelines here link to guidelines . Practicum are activities in the form of a project based on the material covered in the module. The students will work in groups and be expected to deliver a complete assignment in 10 days. There will be two practicums. Assignments The final grade will be calculated using the following weights for each assignment: Exercises There are eight (8) exercises to complete. They will be released at the end of each regular week Lecture and due a week later. The exercises are graded on a scale 1 to 5, where 5 is the highest grade. Discussion Forum There will be a discussion forum the day before the Reading Discussion on Thursday based on the reading from Reading List . All discussions will be on the Ed Platform (select Ed from the tab on the canvas course's main page). Presentations At every Reading Discussion, groups will present the reading material assigned at the beginning of the week. Please see these on the presentations. Practicums There will be two practica during the first two modules (see the schedule for details). Final Projects There will be a final group project due during Exams period encompassing all the material learned in class. Assignment Final Grade Weight Discussion Forum 10% Exercises 10% Presentations 15% Practicums 40% Final Projects 25% Total 100% Getting Help For questions about exercise, course content, package installation, and after you have tried to troubleshoot yourselves, the process to get help is: Go to Office Hours ; this is the best way to get help. Post the question in Ed Forum , and hopefully, your peers will answer. Course Policies Collaboration Policy We encourage students to talk and discuss the assignments with their fellow students. Discussion is encouraged. Presentation during Reading Discussion, Practicum, and Projects are group activities. Communication from Staff to Students Class announcements will be through Ed Forum . Diversity and Inclusion Statement Data Science, like many fields of science, has historically only been represented by a small portion of the population. This is despite some of the pioneers in computer science being from groups that are historically and presently underrepresented. Whenever possible, I will try to highlight the contributions that people have been from a variety of backgrounds. To start, here is a list of some really nice references.: Ada Lovelace Grace Hopper 7 Black Pioneers in Computer Science I welcome any additions to this list you may have! In an ongoing effort to foster a more inclusive environment in computer science, recent initiatives have attempted to overcome some barriers to entry for underrepresented groups: Made w/ Code Women Driven Development BongoHive Like the list above, this list is not exhaustive, but I welcome any additions and suggestions you may have. I would like to attempt to discuss diversity in data science from time to time where appropriate and possible. Please contact me (in person or electronically) or submit anonymous feedback if you have any suggestions to improve the quality of the course materials. The best way to provide anonymous feedback is to use Ed, which allows you to provide comments anonymously. Furthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities (including race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those that appear in your official Harvard records, please let me know! If you feel like your performance in the class is being impacted by your experiences outside of class, please don't hesitate to come and talk with me. I want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to me making a general announcement to the class if necessary to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. I (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it. (Again, anonymous feedback is always an option.) As a participant in course discussions, you should also strive to honor the diversity of your classmates. Academic Honesty Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in AC295 we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the assignments and you should work on all the problems together. Accommodations for students with disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with Pavlos by the end of the third week of the term: Friday, September 18. Failure to do so may result in us being unable to respond in a timely manner. All discussions will remain confidential. Accommodations for students with disabilities Previous Material 2020 Spring","tags":"pages","url":"pages/syllabus.html"},{"title":"Lecture 4: Dask","text":"Slides Lecture 4: Dask | PDF Lecture 4: Dask | PPTX Demo Lecture 4: Dask Data NYC Parking Tickets Dataset Files Dockerfile Dask Demo Readme PDF","tags":"lectures","url":"lectures/lecture4/"},{"title":"Lecture 4: Dask","text":"AC295: Advanced Practical Data Science Lecture 4: Dask Harvard University Spring 2020 Instructors : Pavlos Protopapas Author : Andrea Porelli and Pavlos Protopapas Table of Contents Lecture 4: Dask Part 1: Scalable computing 1.1 Dask API 1.2 Directed acyclical graph (DAGs) ) 1.3 Review Part 1 Part 2: Introduction to DASK 2.1 Exploratory data analysis with DASK 2.2 Visualize directed acyclic graphs (DAGs) ) 2.3 Task scheduling 2.4 Review Part 2 Part 3: Exercise with DASK 3.1 Learn how to Manipulate Structured Data 3.2 Summary part 3 and Dask limitations Part 1: Scalable computing 1.1 Dask API what makes it unique: allows to work with larger datasets making it possible to parallelize computation it is helpful when size increases and even when \"simple\" sorting and aggregating would otherwise spill on persistent memory it simplifies the cost of using more complex infrastructure it is easy to learn for data scientists with a background in the Python (similar syntax) and flexible it helps applying distributed computing to data science project: not of great help for small size datasets: complex operations can be done without spilling to disk and slowing down process. Actually it would generate greater overheads. very useful for medium size dataset; it allows to work with medium size in local machine. Python was not designed to make sharing work between processes on multicore systems particularly easy. As a result, it can be difficult to take advantage of parallelism within Pandas. essential for large datasets: Pandas, NumPy, and scikit-learn are not suitable at all for datasets of this size, because they were not inherently built to operate on distributed datasets. This provide help for using libraries Dataset type Size range Fits in RAM? Fits on local disk? Small dataset Less than 2–4 GB Yes Yes Medium dataset Less than 2 TB No Yes Large dataset Greater than 2 TB No No Adapted from Data Science with Python and Dask Dask consists of several different components and APIs, which can be categorized into three layers: the scheduler: add_definition/explanation; low-level APIs: add_definition/explanation; and high-level APIs: add_definition/explanation . Adapted from Data Science with Python and Dask 1.2 Directed acyclical graph (DAGs) graph : is a representation of a set of objects that have a relationship with one another >>> good to represent a wide variety of information. A graph is compounded by: - *node*: a function, an object or an action - *line*: symbolizes the relationship among nodes directed acyclical graph : there is one logical way to traverse the graph. No node is visited twice. cyclical graph : exists a feedback loop that allow to revisit and repeat the actions within the same node. handle computational resources : as the problem we solve requires more resources we have two options: -*scale up*: increase size of the available resource. invest in more efficient technology, cons diminishing returns -*scale out*: add other resources (dask main idea). invest in more cheap resources, cons distribute workload concurrency : as we approach greater number of \"work to be completed\", some resources might be not fully exploited. For instance some might be idling because of insufficient shared resources (i.e. resource starvation) . Schedulers handle this issue making sure to provide sufficient resources to each worker. failures : - work failures: a worker leave, and you know have to assign another one to his task. This might potentially slowing down the execution, however it won't affect previous work (aka data loss) - data loss: some accident happens and you have to start from the beginning. The scheduler stop and restart from the beginning the whole process. 1.3 Review Part 1 Dask can be used to scale popular Python libraries such as Pandas and NumPy allowing to analyse dataset with greater size (>8GB) Dask uses directed acyclical graph to coordinate execution of parallelized code across processors Directed acyclical graph are made up of nodes, clearly defined start and end that can be transverse in only one logical way (no looping). Upstream actions are completed before downstream nodes. Scaling out (i.e. add workers) can improve performances of complex workloads, however create overhead that can reduces gains. In case of failure, the step to reach a node can be repeated from the beginning without disturbing the rest of the process. Part 2: Introduction to DASK Warm up with short example of data cleaning using Dask DataFrames Visualize Directed Acyclical Graph generated by Dask workloads with graphviz Explore how the scheduler applies the DAGs to coordinate execution code You will learn: Dask DataFrame API Use diagnostic tool Use low-level Delayed API to create custom graph 2.1 Exploratory data analysis with DASK Set up environment and working directory Load data Check data quality issue (e.g. missing values and outliers) Drop columns (not useful for analysis AKA missing many values) 2.1.1 Set up environment and working directory In [1]: # import libraries import sys import os ## import dask libraries import dask.dataframe as dd from dask.diagnostics import ProgressBar # import libraries import pandas as pd In [3]: # assign working directory [CHANGE THIS]. It can not fit in github so I have it locally. Download files os . chdir ( '/nyc-parking-tickets' ) cwd = os . getcwd () # print print ( ' ' , sys . executable ) print ( ' ' , cwd ) /Users/haibui/.pyenv/versions/3.7.7/bin/python3.7 /Users/haibui/00_MIT_Harvard_CS_DS/harvard_data_science/daskdemo/notebook/nyc-parking-tickets 2.1.2 Load data In [4]: ## read data using DataFrame API df = dd . read_csv ( 'Parking_Violations_Issued_-_Fiscal_Year_2017.csv' ) df Out[4]: Dask DataFrame Structure: Summons Number Plate ID Registration State Plate Type Issue Date Violation Code Vehicle Body Type Vehicle Make Issuing Agency Street Code1 Street Code2 Street Code3 Vehicle Expiration Date Violation Location Violation Precinct Issuer Precinct Issuer Code Issuer Command Issuer Squad Violation Time Time First Observed Violation County Violation In Front Of Or Opposite House Number Street Name Intersecting Street Date First Observed Law Section Sub Division Violation Legal Code Days Parking In Effect From Hours In Effect To Hours In Effect Vehicle Color Unregistered Vehicle? Vehicle Year Meter Number Feet From Curb Violation Post Code Violation Description No Standing or Stopping Violation Hydrant Violation Double Parking Violation npartitions=33 int64 object object object object int64 object object object int64 int64 int64 int64 float64 int64 int64 int64 object object object object object object object object object int64 int64 object object object object object object float64 int64 object int64 object object float64 float64 float64 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Dask Name: read-csv, 33 tasks note that metadata are shown in the frame instead of data sample syntax is pretty similar to Pandas API # partitions is the number of splits used to separate the main dataset. The optimal number is decided by the scheduler that split Pandas DataFrame into smaller chuncks. In this case each partitions is ~64MB (i.e. Dataset size/npartitions = 2GB/33). If we have one worker, Dask will cycle to each partition one at time. data types are reported under each column name (similary to describe method in Pandas, however performed through random sampling because data scattered across multiple physical machines). Good practice is to explicit data types instead relying on Dask's inference (store in binary format ideally, see links to Write and Read Parquet). Dask Name reports the name of the DAG (i.e. from-delayed) # tasks is the number of nodes in the DAG. You can think of a task as a Python function and in this case each partition operates 3 tasks: 1) reading the raw data, 2) splitting the data in appropriate blocks and, 3) initialazing the DataFrame object. Every task comes with some overhead (between 200us and 1ms). Partition process from Data Science with Python and Dask Want to know more? Best Practices Write and Read Parquet 2.1.3 Check data quality issue In [5]: # count missing values missing_values = df . isnull () . sum () missing_values Out[5]: Dask Series Structure: npartitions=1 Date First Observed int64 Violation Time ... dtype: int64 Dask Name: dataframe-sum-agg, 100 tasks note that dask object created is a series containing metadata and syntax is pretty similar to Pandas API processing hasn't be completed yet, instead Dask prepared a DAG stored in the missing values variable (advantage of building graph quickly without need to wait for computation) # tasks increased because have been added 2 tasks (i.e. check missing values and sum) for each of the 33 partitions as well as a final addition to aggregate the results among all the partitions for a total of 166 = [(66+1)+99] In [6]: # calculate percent missing values mysize = df . index . size missing_count = (( missing_values / mysize ) * 100 ) missing_count Out[6]: Dask Series Structure: npartitions=1 Date First Observed float64 Violation Time ... dtype: float64 Dask Name: mul, 169 tasks note that dask object created is a series and computation hasn't be completed yet df.index.size is a dask object dask.dataframe.core.Scalar . You cannot access its value/lenght directely like you would do with a list (e.g. len()). It would go against the whole idea of dask (i.e. read all the dataset) # tasks increased because have been added 2 tasks (i.e. division and multiplication) data type ahs changed from int64 to float64. Dask automatically converted it once the datatype at the output might not match the input after the division In [8]: # run computations using compute method with ProgressBar (): missing_count_percent = missing_count . compute () missing_count_percent [################################## ] | 86% Completed | 56.3s /Users/haibui/.pyenv/versions/3.7.7/lib/python3.7/multiprocessing/pool.py:121: DtypeWarning: Columns (18,38) have mixed types.Specify dtype option on import or set low_memory=False. result = (True, func(*args, **kwds)) [########################################] | 100% Completed | 59.5s Out[8]: Summons Number 0.000000 Plate ID 0.006739 Registration State 0.000000 Plate Type 0.000000 Issue Date 0.000000 Violation Code 0.000000 Vehicle Body Type 0.395361 Vehicle Make 0.676199 Issuing Agency 0.000000 Street Code1 0.000000 Street Code2 0.000000 Street Code3 0.000000 Vehicle Expiration Date 0.000000 Violation Location 19.183510 Violation Precinct 0.000000 Issuer Precinct 0.000000 Issuer Code 0.000000 Issuer Command 19.093212 Issuer Squad 19.101506 Violation Time 0.000583 Time First Observed 92.217488 Violation County 0.366073 Violation In Front Of Or Opposite 20.005826 House Number 21.184968 Street Name 0.037110 Intersecting Street 68.827675 Date First Observed 0.000000 Law Section 0.000000 Sub Division 0.007155 Violation Legal Code 80.906214 Days Parking In Effect 25.107923 From Hours In Effect 50.457575 To Hours In Effect 50.457548 Vehicle Color 1.410179 Unregistered Vehicle? 89.562223 Vehicle Year 0.000000 Meter Number 83.472476 Feet From Curb 0.000000 Violation Post Code 29.530489 Violation Description 10.436611 No Standing or Stopping Violation 100.000000 Hydrant Violation 100.000000 Double Parking Violation 100.000000 dtype: float64 note that .compute() method is necessary to run the actions embedded in each node of the DAG The results of the compute method are stored into a Pandas Series ProgressBar() is a wrapper to keep track of running tasks. It shows completed work from a quick visual inspection we can see that are columns that are incomplete and we should drop 2.1.4 Drop columns In [9]: # filter sparse columns(greater than 60% missing values) and store them columns_to_drop = missing_count_percent [ missing_count_percent > 60 ] . index print ( columns_to_drop ) # drop sparse columns with ProgressBar (): #df_dropped = df.drop(columns_to_drop, axis=1).persist() df_dropped = df . drop ( columns_to_drop , axis = 1 ) . compute () Index(['Time First Observed', 'Intersecting Street', 'Violation Legal Code', 'Unregistered Vehicle?', 'Meter Number', 'No Standing or Stopping Violation', 'Hydrant Violation', 'Double Parking Violation'], dtype='object') [##################################### ] | 93% Completed | 1min 5.2s /Users/haibui/.pyenv/versions/3.7.7/lib/python3.7/site-packages/dask/core.py:121: DtypeWarning: Columns (18,38) have mixed types.Specify dtype option on import or set low_memory=False. return func(*(_execute_task(a, cache) for a in args)) [########################################] | 100% Completed | 1min 6.2s note that use Pandas Series to drop columns in Dask DataFrames because each partition is a Pandas DataFrame In the case the Series is made available to all threads, in a cluster it would be serialized and brodcast to all the worker nodes .persit() method allows to store in memory intermediate computations so they can be reused. 2.2 Visualize directed acyclic graphs (DAGs) DASK uses graphviz library to generate visual representation of the DAGs created by the scheduler Use .visualize() metod to inspect the DAGs of DataFrames, Series, Bag, and arrays For simplicity we will use Dask Delayed object instead of DataFrames since they grow quite large and hard to visualize delayed is a constructor that allows to wrap functions and create Dask Delayed objects that are equivalent to a node in a DAG. By chaining together delayed object, you create the DAG Below are two examples, in the first one you build a DAG with one node only with dependencies and in the second one you build another with multiple nodes with dependencies 2.2.1 Example 1: DAG with one node with depenndecy In [16]: # import library import dask.delayed as delayed In [17]: def increment ( i ): return i + 1 def add ( x , y ): return x + y # wrap functions within dealyed object and chain x = delayed ( increment )( 1 ) y = delayed ( increment )( 2 ) z = delayed ( add )( x , y ) # visualize the DAG z . visualize () Out[17]: In [18]: # show the result z . compute () Out[18]: 5 note that to build a node wrap the function with the delayed object and then pass the arguments of the function. You could also use a decoretors (see documentation). circles symbolize function and computations while squares intermediate or final result incoming arrows represent dependecies. The increment function do not have any dependency while add function has two. Thus, add function has to wait until objects x and y have been calculated functions without dependencies can be computed independentely and a worker can be assigned to each one of them use method .visualize() on the last node with dependencies to peak at the DAG Dask does not compute the DAG . Use the method .compute() on the last node to see the result 2.2.2 Example 1: DAG with more than one node with dependencies we are going to build a more complex DAG with two layers: layer1 is built by looping over a list of data using a list comprehension to create dask delayed objects as \"leaves\" node. This layer combine the previously created function increment with the values in the list, then use the built in function sum to combine the results; layer2 is built looping on each object created in layer1 and In [19]: data = [ 1 , 2 , 3 , 4 , 5 ] # compile first layer and visualize layer1 = [ delayed ( increment )( i ) for i in data ] total1 = delayed ( sum )( layer1 ) total1 . visualize () Out[19]: In [20]: def double ( x ): return x * 2 # compile second layer and visualize layer2 = [ delayed ( double )( j ) for j in layer1 ] total2 = delayed ( sum )( layer2 ) #.persist() total2 . visualize () Out[20]: In [21]: z = total2 . compute () z Out[21]: 40 note that built in function persistent using .persist() and it will be represented as a rectangle in the graph In [22]: ## TASK # visualize DAGs built from the DataFrame missing_count . visualize () Out[22]: Want to know more? Parallelize non DataFrame code 2.3 Task scheduling Dask performs the so called lazy computations. Remember, until you run the method .compute() all what Dask does is to split the process into smaller logical pieces (which avoid loading all the dataset) Even though the process is defined, the number of resources assigned and the place where the results will be stored are note assigned because the scheduler assign them dynamically. This allow to recover from worker failure, network unreliability as well as workers completing the tasks at different speeds. Dask uses a central scheduler to orchestrate all this work. It splits the workload among different servers which unlikely they are perfectly assigned the same load, power or access to data. Due to these conditions, scheduler needs to promptly react to avoid bottlenecks that will affect overall runtime. For best performance, a Dask cluster should use a distributed file system (S3, HDFS) to back its data storage. Assuming there are two nodes like in the image below and data are stored in one. In order to perform computation in the other node we have to move the data from one to the other creating an overhead proportional to the size of the data. The remedy is to split data minimizing the number of data to broadcast across different local machines. Local disk from Data Science with Python and Dask Dask scheduler takes data locality into consideration when deciding where to perform computation. 2.4 Review Part 2 The beauty of this code is that you can reuse in one machine or a thousand Similar syntax helps transition from Pandas to Dask (good in general for refactoring your code) Dask DataFrames is a tool to parallelize computing done popular Pandas that allow to clean and ananlyze large dataset Dask parallize Pandas Dataframe and more in general work using DAGs Computation are structured by the task scheduler using DAGs Computation are constructed lazily and then compute the method Use visualize method for a visual representation Computation can persist in memory avoid slowdown to replicate result Data locality help minimizing network and IO latency Part 3: Exercise with DASK 3.1 Learn how to manipulate structured data Build a Dask DataFrame from a Pandas DataFrame In [23]: # create lists with actions action_IDs = [ i for i in range ( 0 , 10 )] action_description = [ 'Clean dish' , 'Dress up' , 'Wash clothes' , 'Take shower' , 'Groceries' , 'Take shower' , 'Dress up' , 'Gym' , 'Take shower' , 'Movie' ] action_date = [ '2020-1-16' , '2020-1-16' , '2020-1-16' , '2020-1-16' , '2020-1-16' , '2020-1-17' , '2020-1-17' , '2020-1-17' , '2020-1-17' , '2020-1-17' ] # store list into Pandas DataFrame action_pandas_df = pd . DataFrame ({ 'Action ID' : action_IDs , 'Action Description' : action_description , 'Date' : action_date }, columns = [ 'Action ID' , 'Action Description' , 'Date' ]) action_pandas_df Out[23]: Action ID Action Description Date 0 0 Clean dish 2020-1-16 1 1 Dress up 2020-1-16 2 2 Wash clothes 2020-1-16 3 3 Take shower 2020-1-16 4 4 Groceries 2020-1-16 5 5 Take shower 2020-1-17 6 6 Dress up 2020-1-17 7 7 Gym 2020-1-17 8 8 Take shower 2020-1-17 9 9 Movie 2020-1-17 In [24]: # convert Pandas DataFrame to a Dask DataFrame action_dask_df = dd . from_pandas ( action_pandas_df , npartitions = 3 ) In [25]: action_dask_df Out[25]: Dask DataFrame Structure: Action ID Action Description Date npartitions=3 0 int64 object object 4 ... ... ... 8 ... ... ... 9 ... ... ... Dask Name: from_pandas, 3 tasks In [26]: # info Dask Dataframe print ( ' ' , action_dask_df . divisions ) print ( '<# partitions>' , action_dask_df . npartitions ) (0, 4, 8, 9) <# partitions> 3 In [27]: # count rows per partition action_dask_df . map_partitions ( len ) . compute () Out[27]: 0 4 1 4 2 2 dtype: int64 In [28]: # filter entries in dask dataframe print ( ' \\n ' ) action_filtered = action_dask_df [ action_dask_df [ 'Action Description' ] != 'Take shower' ] print ( action_filtered . map_partitions ( len ) . compute ()) print ( ' \\n ' ) action_filtered_reduced = action_filtered . repartition ( npartitions = 1 ) print ( action_filtered_reduced . map_partitions ( len ) . compute ()) 0 3 1 3 2 1 dtype: int64 0 7 dtype: int64 3.2 Summary part 3 and dask limitations dataframe are immutable. Functions such as pop and insert are not supported does not allow for functions with a lot of shuffeling like stack/unstack and melt limit this operations after major filter and preprocessing join, merge, groupby, and rolling are supported but expensive due to shuffeling reset index starts sequential counting for each partitions apply and iterrow are known to be inefficient in Pandas, the same for Dask use .division() to inspect how DataFrame has been partitioned for best performance partitions should be rougly equal. Use .repartition() method to balance across datasets for best performance sort by logical columns, partition by index and index should be presorted In [ ]: In [ ]:","tags":"Lectures","url":"lectures/lecture4/demo/"},{"title":"Reading Discussion 3","text":"Selected Readings Expository Why every Data Scientist should use Dask? Pandas, Dask or PySpark? What Should You Choose for Your Dataset? Dask Tutorial Dask hyperparameter optimization t-SNE with GPUs Use Cases * ReproduceIt reddit word count Deploy Dask using SSH cluster - There's a corresponding video - Build a dask cluster from couple of laptops at home Visualization with Dask - Datashader - Choose one Dask use cases - Choose one Research * Dask: Parallel Computation with Blocked algorithms and Task Scheduling Composable Multi-Threading and Multi-Processing for Numeric Libraries A performance comparison of Dask and Apache Spark for data-intensive neuroimaging pipelines Better and faster hyperparameter optimization with Dask Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence - Section 4 * Next presentations, select from Research or Use Cases","tags":"readings","url":"readings/reading3/"},{"title":"Lecture 3: Kubernetes","text":"Slides Lecture 3: Kubernetes | PDF Lecture 3: Kubernetes | PPTX Demo Demo : WebDB K8s Files hello_world_db_deployment.yml hello_world_server_deployment.yml","tags":"lectures","url":"lectures/lecture3/"},{"title":"Reading Discussion 2","text":"Kubernetes [9/10 not a complete list] Expository Kubernetes 101 Kubernetes Architecture 101 The Kubernetes Bible for Beginners & Developers Kubeflow Benefits of Kubernetes Why Kubernetes is a Considerable Step Forward in Software Products Development? Local Kubernetes Development Containers and Cloud: From LXC to Docker to Kubernetes Kubernetes Virtual Clusters for AI/ML Experiments ) Antifragility in Kubernetes Longer Reads Kubernetes: Up and Running (ch.12-14) Hands-On Docker for Microservices with Python: design, deploy, and operate a complex system with multiple microservices using Docker and Kubernetes (ch.5-7) Research Firmament: Fast, Centralized Cluster Scheduling at Scale Democratizing Production: Scale Distributed Deep Learning Serverless inferencing on Kubernetes Use cases and other interesting reads Kubeflow: Build an ML pipeline Poseidon-Firmament, The package for Kubernetes Scheduler Helm, The package manager for Kubernetes Skaffold, The package for Kubernetes local development Loft, The package for Kubernetes virtual cluster Virtual Cluster: Sandboxes for ML experiment Choose from Research and Use cases","tags":"readings","url":"readings/reading2/"},{"title":"Lecture 2: Containers","text":"Slides Lecture 2: Virtual Environments, Virtual Machines, and Containers | PDF Lecture 2: Virtual Environments, Virtual Machines, and Containers | PPTX Demo Demo 1: Getting started with dockers Demo 2: Web-db dockerized Files syllabus.txt hello_world_server.py hello_world_db.py Dockerfile_db Dockerfile_server Docker cheat sheet Docker cheat sheet","tags":"lectures","url":"lectures/lecture2/"},{"title":"Lecture 2: Demo2","text":"APCOMP 295 Advanced Practical Data Science Demo 1 - Getting started with Docker Harvard University Fall 2020 Instructors : Pavlos Protopapas Installing Docker Desktop Install Docker Desktop . Use one of the links below to download the proper Docker application depending on your operating system. For Mac users, follow this link- https://docs.docker.com/docker-for-mac/install/ . For Windows users, follow this link- https://docs.docker.com/docker-for-windows/install/ Note: You will need to install Hyper-V to get Docker to work. For Linux users, follow this link- https://docs.docker.com/install/linux/docker-ce/ubuntu/ Once installed run the docker desktop. Open a Terminal window and type docker run hello-world to make sure Docker is installed properly. [If you are working on a Windows machine, we are aware of some issues with downloading and installing Docker Desktop depending on your version of Windows.] Building a Dockerfile for a Linux container Create a lecture2/ directory. Download the syllabus.md file from our web page to your lecture2/ directory. Note: Use curl https://raw.githubusercontent.com/Harvard-IACS/2020F-AC295/master/content/pages/syllabus.md -o syllabus.md to download the syllabus.md file from the web-page. Do not copy and paste into a file on your computer. This will give different character counts than if you download it. Open a new file called Dockerfile in a text editor. Save the file to your lecture2/ directory. Copy and paste the following lines into your Dockerfile . # The following line will download the latest Ubuntu Docker image # to use as a foundation for building our own Docker image. FROM ubuntu:latest RUN apt-get update # The following line will copy the syllabus.md file to the Docker container # using the path that is specified second. COPY syllabus.md /syllabus.md Run docker build -t syllabus:first . from your lecture2/ directory, which should contain your Dockerfile and syllabus.md . - Note there is an \".\" at the end of the line. - The `-t` flag tells the `build` command to tag your Docker image with a specific name. Check out this [link](https://www.freecodecamp.org/news/an-introduction-to-docker-tags-9b5395636c2a/) for an in-depth explanation of Docker tags. - If you choose to name the Dockerfile something else you could us: `docker build -t syllabus:first -f filename .`. Run docker image ls . You should see your docker image at the top of the list. You should also see the ubuntu image we used to create our docker image in the list. Run a docker container Now we are ready to start up a Docker container using the image we built. Run docker run -it syllabus:first in your terminal window. - The combination of the flags `-it` will open an interactive shell in the Docker container. - You should see that your command-line prompt changes to `root@...`. You're now in the Docker container, which is running Linux. Use ls and ensure the syllabus.md file is in your container. In another terminal window (not the one connected to your Docker container), type docker ps . You should see that you have one container that is running and that it was created a few minutes ago. Let's count the lines that contain a numerical digit in syllabus.md . To do so run the command grep [0-9] syllabus.md . Depending on your OS will give you different answers. If you have a MacOS, you should see 52 . If you have Windows or Linux, then you should see 51 . Now, run grep [0-9] syllabus.md | wc -l in the Docker container. Exit and deleting Type exit in the Docker container window to leave the container. Type docker ps again (in any terminal window) and you should see that there are no containers running. If you would like to clean up your local Docker registry to remove images and stopped containers, follow these instructions . In [ ]:","tags":"Lectures","url":"lectures/lecture2/demos1/"},{"title":"Lecture 2: Demo2","text":"APCOMP 295 Advanced Practical Data Science Demo 2 - Web DB servers Harvard University Fall 2020 Instructors : Pavlos Protopapas We will build a simple Python web application that returns \"Hello world\" in a few different languages when you visit the web page. Build a Python web application locally Working off of your lecture2/ directory you build in the previous demo. Download the hello_world_server.py file from the AC295 web page and place it in your lecture2/ directory. If you are running Python 3, then you should be able to run the web application using python hello_world_server.py in your terminal window. Visit http://localhost:8082/ in a web browser to make sure your application is up and running. You should get a web page that says Could not connect to database. This is because our web server is trying to connect to a \"database\" to retrieve information about what should be displayed on the web page, but we have not started up our database server. Let's start up a database that our hello world server can connect to. This requires us to start up a second web server that will host our database. In this example, our database is really just a Python list. Please download the hello_world_db.py file from the AC295 web page and place it in your lecture2/ directory. Take a look at the .py file to see that the database is just a Python list. Open a second Terminal window and change directories into your lecture2/ directory and run python hello_world_db.py in your terminal window. You should see a message beginning with \"Created HTTP server to run our database...\", which means you have successfully started a web server with a \"database\". Now, go back to http://localhost:8082/ in a web browser. You should see a web page that says \"Hello world!\" or its translation in Spanish, French, German, Mandarin, or Hindi. If you reload the page several times, you should see the translation changes. Although this is a simple example, you are running a fully functioning web application using two individual web servers on your computer! Below is a schematic of the web application you have built. The servers are running on top of your OS, using your native Python. Because you are running the servers locally on your laptop, you are restricted to the localhost IP address to connect the two servers. As a result, a user can technically access your database directly by sending a request to http://localhost:8081 , which is not what you want. Dockerize your Python web application Going forward in this tutorial will require that you have Docker Desktop downloaded on your machine. If you were unable to download Docker Destop in the previous demo, please complete the rest of this exercise with a partner. We will now put our \"Hello world\" python web application into 2 individual Docker containers. First, we must create a network on which our Docker containers can communicate with each other. Run docker network create helloNetwork . Run docker network ls . You should see helloNetwork in the list. Let's create a Docker container to run our database server now. Open a new Terminal window and change directories to the lecture2/ directory. Please download the Dockerfile_db file from the AC295 web page and place it in your lecture2/ directory. Run docker build -t ac295-lecture2:db -f Dockerfile_db . to build a Docker image for our database server. If you look at the build log, you will see that this Docker image also installs numpy just for the container. This is useful in the situation that different parts of our application rely on different versions of numpy or some other package. This encapsulation and isolation actually allows for more efficient development of individual components of an application because the back-end of the components can be changed and upgraded independently as long as the communication lines (API calls between the components remain consistent. Run docker image ls to see that we created a new Docker image with the tag db . Run docker run --name db -d --network helloNetwork ac295-lecture2:db in your terminal window. The --name flag allows us to name the container. The -d flag tells Docker to run this container in the background. The --network flag tells Docker which network we want the container to be connected to. Isolating containers to a specific network allows us to provide singular communication lines between different parts of an application and can prevent unwanted breaches. In this case, the container we just started is our database server and we do not want people on the outside to have access to it. Try going to http://localhost:8081/ to see if you can access the database server (hint: you should get a connection error). Check that our container is connected to the helloNetwork we created by running docker network inspect helloNetwork . You should see a dictionary with your Docker container. Let's create a Docker container for our front-end web server now. Please download the Dockerfile_server file from the AC295 web page and place it in your lecture2/ directory. Run docker build -t ac295-lecture2:server -f Dockerfile_server . Run docker image ls to see that we created a new Docker image with the tag server . Run docker run --name webServer -d -p 8082:8082 -e DB_URL=http://db:8081 --network helloNetwork ac295-lecture2:server . The -e flag allows us to specify an environment variable for our Docker container. In this case, we specify the URL through which our front-end server should communicate with our database server on the private helloNetwork network. If you look at Dockerfile_server file, you will see that it defines the DB_URL and passes it to the Python call to hello_world_server.py . By using the -p 8082:8082 flag, we expose port 8082 to the outside so our web page can be accessed. If visit http://localhost:8082/ , you will see that our \"Hello world\" application is working again. If you refresh the page a few times, you should see \"Hello world!\" in different languages. Below is a schematic of your Dockerized web application. The two servers are running in individual Docker containers on top of your OS. Since we created a separate network for our containers to communicate on, you can no longer directly access your database by sending a request to http://localhost:8081 . This is the type of encapsulation you would like for your database because it generally stores sensitive information. Clean-up steps: Stop your containers with docker stop $(docker container ls -q) . This may take a few seconds. Delete your containers with docker rm $(docker ps -aq) . Beware, this will stop all containers you have running on your computer, so if you have containers running for other classes, you will have to remove the containers using the container IDs. Use docker rmi to delete your images, if you would like to. In [ ]:","tags":"Lectures","url":"lectures/lecture2/demos2/"},{"title":"Lecture 2: docker-cheatsheet","text":"Docker Cheatsheet docker image Build an image from a Dockerfile Tag an image List images Remove an image Synchronise with a container registry docker container Create - but do not start - a container from an image Start a container Create and start a container from an image Stop a running container Pause or unpause all running processes in a container Delete a container List containers Copy files Execute a command in a container Create an image from a container Attach terminal to container Display container information Print container logs docker network Create a network List networks Delete a network Connect or disconnect a container from a network Dockerfile instructions Set the base image Run a command in the shell during the build stage Publish a port Set a variable for use during the build stage Set an environment variable Set the working directory Copy files into a container Set the default command for a running container docker image Build an image from a Dockerfile docker image build -t IMAGE_NAME:TAG PATH_TO_BUILD_CONTEXT The Dockerfile should be located at the root of the build context and should be called Dockerfile . If it is not, use docker build -t -IMAGE_NAME:TAG -f PATH_TO_DOCKERFILE PATH_TO_BUILD_CONTEXT PATH_TO_DOCKERFILE cannot be outside of the build context . Tag an image To create a tag TARGET_IMAGE that refers to SOURCE_IMAGE , use docker image tag SOURCE_IMAGE:TAG TARGET_IMAGE:TAG List images docker image ls Include intermediate images docker image ls -a Remove an image docker image rm IMAGE_NAME:TAG Remove danging intermediate images docker image prune Remove images not referenced by a container docker image prune -a Synchronise with a container registry Pull from the registry docker image pull IMAGE_NAME:TAG Push to the registry docker image push IMAGE_NAME:TAG docker container Create - but do not start - a container from an image docker container create IMAGE_NAME:TAG Publish ports docker container create -p HOST_PORT:CONTAINER_PORT/PROTOCOL IMAGE_NAME:TAG HOST_PORT is the port used to access the container from outside. CONTAINER_PORT is the port used within the container. PROTOCOL must be either tcp or udp . If PROTOCOL is not specified, tcp is assumed. Mount a volume docker container create --mount source=PATH_ON_DISK, target=PATH_IN_CONTAINER, type=bind IMAGE_NAME:TAG Start a container docker container start CONTAINER_NAME Print container output docker container start -a CONTAINER_NAME Connect to the container's terminal docker container start -it CONTAINER_NAME CTRL+P then CTRL+Q to disconnect. Create and start a container from an image docker container run IMAGE_NAME:TAG docker container run combines docker container create and docker container start . Publish ports docker container run -p HOST_PORT:CONTAINER_PORT/PROTOCOL IMAGE_NAME:TAG HOST_PORT is the port used to access the container from outside. CONTAINER_PORT is the port used within the container. PROTOCOL must be either tcp or udp . If PROTOCOL is not specified, tcp is assumed. Mount a volume docker container run --mount source=PATH_ON_DISK, target=PATH_IN_CONTAINER, type=bind IMAGE_NAME:TAG Stop a running container Shutdown gracefully docker container stop CONTAINER_NAME Stop immediately docker container kill CONTAINER_NAME Pause or unpause all running processes in a container Pause docker container pause CONTAINER_NAME Unpause docker container unpause CONTAINER_NAME Delete a container docker container rm CONTAINER_NAME Stop and delete a running container docker container rm -f CONTAINER_NAME Stops immediately, no graceful shutdown. Delete all stopped containers docker container prune Delete all running and stopped containers docker container rm -f $(docker ps -aq) List containers Running containers docker container ls All containers docker container ls --all Copy files To container docker container cp PATH_ON_DISK CONTAINER_NAME:PATH_IN_CONTAINER From container docker container cp CONTAINER_NAME:PATH_IN_CONTAINER PATH_ON_DISK Execute a command in a container docker container exec CONTAINER_NAME COMMAND OPTIONAL_ARGUMENTS Run in background docker container exec -d CONTAINER_NAME COMMAND OPTIONAL_ARGUMENTS Execute with temporary environment variables docker container exec -e VARIABLE_NAME=VALUE CONTAINER_NAME COMMAND OPTIONAL_ARGUMENTS Create an image from a container docker container commit CONTAINER_NAME Attach terminal to container docker container attach CONTAINER_NAME CTRL+C to detach. Display container information docker container inspect CONTAINER_NAME Print container logs docker container logs CONTAINER_NAME Print the last N lines of the container logs docker container logs --tail N CONTAINER_NAME Print all subsequent logs docker container logs -f CONTAINER_NAME docker network Create a network docker network create NETWORK_NAME List networks docker network ls Delete a network docker network rm NETWORK_NAME Remove all unused networks docker network prune Connect or disconnect a container from a network Connect docker network connect NETWORK_NAME CONTAINER_NAME Disconnect docker network disconnect NETWORK_NAME CONTAINER_NAME Dockerfile instructions To build a Dockerfile, use docker image build -t IMAGE_NAME:TAG PATH_TO_BUILD_CONTEXT The Dockerfile should be located at the root of the build context and should be called Dockerfile . If it is not, use docker build -t -IMAGE_NAME:TAG -f PATH_TO_DOCKERFILE PATH_TO_BUILD_CONTEXT PATH_TO_DOCKERFILE cannot be outside of the build context . docker image build executes each instruction in the Dockerfile in sequence. After each instruction, the resulting intermediate image is committed. If docker image build is re-run using the same build context, cached images are used until the first changed instruction. Consequently, it is good practice to structure the Dockerfile so that instructions that change less frequently are executed first. Build context The Dockerfile and any local files copied to the container must be contained within a build context. The build context is a local directory. When docker image build is run, the entire build context is copied to the Docker daemon. The exception to this are files and directories that are marked for omission in a .dockerignore file that is placed at the root of the build context. Because large build contexts reduce the performance of docker image build , it is good practice to only include directories and files that are relevant to the container in the build context or to use a .dockerignore file. Set the base image FROM IMAGE_NAME:TAG This must be the first instruction in a Dockerfile. (The only exception is the ARG instruction, which may be used to set IMAGE_NAME or TAG . Variables set before the FROM instruction do not persist after it .) Run a command in the shell during the build stage RUN COMMAND OPTIONAL_ARGUMENTS This runs commands using the default shell ( /bin/sh -c on Linux, cmd /S /C on Windows). This is known as the shell form . An alternative form is the exec form : RUN [\"EXECUTABLE\", \"OPTIONAL_ARGUMENT1\", \"OPTIONAL_ARGUMENT2\", ...] Publish a port RUN -p HOST_PORT:CONTAINER_PORT/PROTOCOL HOST_PORT is the port used to access the container from outside. CONTAINER_PORT is the port used within the container. PROTOCOL must be either tcp or udp . If PROTOCOL is not specified, tcp is assumed. Set a variable for use during the build stage ARG KEY=VALUE If VALUE contains spaces, enclose it within double-quotes or precede each space with a backslash. To reference this variable, use $KEY . Multiple variables can be set in a single instruction. To do this, use ARG KEY1=VALUE1 KEY2=VALUE2, ... Variables set using ARG do not persist once the container has been built. To set an environment variable that is available when the container is running, use ENV . Set an environment variable ENV KEY=VALUE If VALUE contains spaces, enclose it within double-quotes or precede each space with a backslash. Variables set using ENV can be referenced in a Dockerfile - and override variables set using ARG . To reference a variable, use $KEY . Multiple variables can be set in a single instruction. To do this, use ENV KEY1=VALUE1 KEY2=VALUE2, ... Variables set using ENV persist when the container is running. Set the working directory To set the working directory for subsequent RUN , COPY , ADD and CMD instructions, use WORKDIR PATH_IN_CONTAINER If the WORKDIR instruction is used multiple times, each subsequent path will be set relative to the path of the previous WORKDIR instruction. Copy files into a container Copy files into a container from a local source COPY PATH_ON_DISK PATH_IN_CONTAINER PATH_ON_DISK cannot be outside of the build context . (This is usually the same directory as the Dockerfile .) Copy files into a container from any source ADD SOURCE_PATH PATH_IN_CONTAINER SOURCE_PATH can be a local file or a URL. If it is a local tar archive with extensions gzip, bzip2 or xz, it is first unpacked. If SOURCE_PATH is a local file it cannot be outside of the build context . (This is usually the same directory as the Dockerfile .) ADD is more powerful than COPY - it supports both local and remote sources and unpacks tar archives. When copying files from a local source, COPY is preferred because it is more transparent . Set the default command for a running container CMD [\"EXECUTABLE\", \"OPTIONAL_ARGUMENT1\", \"OPTIONAL_ARGUMENT2\", ...] This is the exec form , which is preferred. However, to pass an environment variable, it is necessary to use the shell form : CMD COMMAND OPTIONAL_ARGUMENTS There can only be one CMD instruction in a Dockerfile. If CMD is used more than once, only the last instruction is effective. Made with ❤ by Javid Lakha for the Harvard AC295 / CSCI E-115 students. Please send suggestions and corrections to javid@hey.com . Version 1.2.1","tags":"Lectures","url":"lectures/lecture2/docker-cheatsheet/"},{"title":"Lecture 3: Demo","text":"APCOMP 295 Advanced Practical Data Science Demo 3 - Web DB servers on minikube Harvard University Fall 2020 Instructors : Pavlos Protopapas Deploy your container to a Kubernetes Cluster Last week we built a dockerized web + db application now the goal is to deploy it on Kubernetes Cluster. Prerequites You will need to have installed kubectl and Minikube according to the instructions. This part of the Kubernetes tutorial was adapted from https://medium.com/@yzhong.cs/getting-started-with-kubernetes-and-docker-with-minikube-b413d4deeb92 . Setup Run minikube start in your Terminal window. You should see several lines beginning with emojis. The last line should say Done! kubectl is now configured to use \"minikube\" . You now have a virtual cluster running on your machine/computer. You can think of this cluster as running a virtual machine on your personal computer. Kubernetes organizes Docker containers into Pods . Docker containers in the same pod share CPU allocation and memory. Typically, you would want multiple Docker containers in the same pod because they must interact to achieve some process, such as dealing with reads and writes to a database. Some info on why a pod may have multiple containers . To create a pod, we must also create a Deployment , which is basically a set of rules for how much CPU and memory a pod should have access to and different labels/names for a pod. Additionally, a deployment specifies what should happen to a pod if it stops running. Deploy DB Deployments can be configured from the command-line, but this becomes difficult when you have many parameters to specify. Therefore, users typically generate a YAML file specifying the details of their deployment, which is what we will do as well. It is convention to name the deployment object the same as the YAML file. Please download the hello_world_db_deployment.yml file from the course website and place it in the same directory you created for lecture2. The name of our Deployment is hello-world-db-deployment . We only want one pod for our deployment, as indicated by replicas: 1 . The selector defines how the Deployment finds which Pod(s) to manage. In this case, we simply select a label that is defined in the Pod template. That's what the two books-app fields are for. We specify the Docker image and version we want to use- pavlosprotopapas/ac295_lecture2:db . The imagePullPolicy is set to Always since we want to pull the Docker image from Docker Hub whenever we create a new Pod. I already pushed our database server Docker image to a repository on Docker hub, making it easier for us to deploy the Docker image with our web application using Kubernetes. You can see that we expose port 8081 for the Pod. We can create a deployment for our Docker image using kubectl create -f hello_world_db_deployment.yml . You should get a message saying \"deployment.apps/hello-world created\". You can now use the kubectl get deployments command to see that your deployment is available, meaning it is ready to receive HTTP requests. Use kubectl get pods command to see that pods are running. Deploy Server Now, the Pod is running our database server running on our Kubernetes cluster, however this server is not exposed to the outside world because it is encapsulated within the cluster. Check this by trying to connect to http://localhost:8081/ . You should get a connection error. We need to now create a deployment for our front-end web server, so that we can access our web application. Let's create a deployment for our front-end server. Please download the hello_world_server_deployment.yml file from the course website and place it in the same directory. Remember that we need to have our front-end server connect to our database server, like we accomplished when we were just running the Docker containers on our local machines. To accomplish this, we need to know the internal IP address that Kubernetes has assigned to the Pod running our database server. This IP address can only be used from within the Kubernetes cluster. We can get this IP by running kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP . Open up the hello_world_server_deployment.yml file and make sure the IP you get for the hello-world-db-deployment is the same IP address specified in the value field for the DB_URL . If it is the same, then you're good to go. If it is different, then please change it. You will also see that we specify port 8080 as a communication port for the Pod. Now, we can run kubectl create -f hello_world_server_deployment.yml to create a deployment for our front-end web server. Use kubectl get deployments and kubectl get pods to see that your pods are running. Now that we have both parts of our application running, try visiting http://localhost:8080/ . Can you access the web application? The answer should be no because although we have exposed ports for our Pods, they are not accessible to the outside world. So far, we can only communicate with the Pods running our application if we are within the Kubernetes cluster (again, this is for isolation purposes). Let's check this out from within the cluster. Run kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP and copy the IP address for the hello-world-server-deployment . Run minikube ssh in your Terminal window. You should see \"minikube\" written in you Terminal window. Run curl :8080/ several times. You should get responses for \"Hello world!\" in different languages, demonstrating that the application is working. Run exit to exit the minikube Terminal. Expose to the world To be able to access our web application from outisde, we need to create a Kubernetes Service to make the hello-world container accessible from outside the Kubernetes virtual network. Use kubectl expose deployment hello-world-server-deployment --type=LoadBalancer --port=8080 to allow your container to receive HTTP request from outside. You should get a message that says \"service/hello-world exposed\". You can view the status of your sercice by using the kubectl get services command. Notice that the EXTERNAL-IP for our service is pending . If we were running our Kubernetes cluster using a cloud provider, such as AWS, we would get an actual IP address to access our service from anywhere. Since we are running Minikube, you can access your service by using the minikube service hello-world-server-deployment . This should automatically open a web page with our Hello world! page. Reload this page a few times to see the different \"Hello world!\" translations. Congratulations! You have deployed a web application using Kubernetes! Below is a schematic of your Dockerized web application on a Kubernetes (K8s) cluster. The two servers are running in individual Docker containers within the Kubernetes cluster, which is emulated by a VM in your case. Generally, a Kubernetes cluster would exists across serveral machines on a cloud provider like AWS. The K8s cluster takes care of generating a network for us and provides individual IP addresses for each of our Pods, reducing our work. By creating a K8s service for your web application, you provide outside users an entry point to use your application while protecting your database. Clean Up You can clean up the resources and cluster using: 1. `kubectl delete service hello-world-server-deployment` 2. `kubectl delete deployment hello-world-server-deployment` 2. `kubectl delete deployment hello-world-db-deployment` 3. `minikube stop` 3. `minikube delete` In [ ]:","tags":"Lectures","url":"lectures/lecture3/demo/"},{"title":"Lecture 1: Introduction: Virtual Enviroments and Virtual Boxes","text":"Slides Lecture 1: Introduction | PDF Lecture 1: Introduction | PPTX","tags":"lectures","url":"lectures/lecture1/"},{"title":"Reading Discussion 1","text":"Selected Readings Virtual Environments Expository Getting Started with Python Virtual Environments - Venv Getting started with Python Virtual Environments using Conda: Why Getting started with Python Virtual Environments using Conda: How Virtual Machines Expository How to Install a Windows 10 on Your Mac Enable Virtual Machines in Windows 10 Dockers Expository Make Code Accessible with these Cloud Services Install Dockers on Windows 10 Home Dockers Getting Started Docker Best Practices Docker WorkShop for Beginners Docker Labs Software Development with Docker Research * Docker: Lightweight Linux Containers for Consistent Development and Deployment Comparison of different Linux containers Singularity: Scientific containers for mobility of compute Use Cases: Dockers in a real setting * The BinderHub Architecture Explained DIY: Build your Binder Client/Server-Side Application Next presentations, select from Research or Use Cases","tags":"readings","url":"readings/reading1/"},{"title":"Topics in Applied Computation: Advanced Practical Data Science","text":"Fall 2020 Pavlos Protopapas Office Hours: By appointment Course helpline: ac295f2020@gmail.com Welcome to AC295: advanced practical data science. The course will be divided into three major topics: 1. How to scale a model from a prototype (often in jupyter notebooks) to the cloud. In this module, we cover virtual environments, containers, and virtual machines before learning about containers and Kubernetes. Along the way, students will be exposed to Dask. 2. How to use existing models for transfer learning. Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. It is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks. This could be very important, given the vast computing resources required to develop neural network models on these problems and the huge gains that these models can provide. In this part of the course, we will examine various pre-existing models and techniques in transfer learning. 3. In the third part, we will introduce several intuitive visualization tools for investigating and diagnosing models. We will be demonstrating a number of visualization tools ranging from the well established (like saliency maps) to recent examples that have appeared in https://distill.pub. Lectures (online): Tuesday and Thursday 10:30-11:45am (and possibly depending on timezone of students repeat Tuesday and Thursday from 6:00-7:15pm) Office Hours: (all times EST) (Office hours begin 09/08) TF Day Office Hours William Palmer Sunday 10:00-11:30 AM Shivas Jayaram Sunday 8:00 - 9:30 PM Javid Lakha Monday 4:30 - 6:00 PM Faras Sadek Monday 6:00 - 7:30 PM Hai Bui Wednesday 11:00 AM - 12:30 PM Andrea Porelli Thursday 1:00 -2:30 PM Rashmi Banthia Friday 8:30-10:00 AM Yujiao Chen Saturday 11:00 AM - 12:30 PM Previous Material 2020 Spring","tags":"pages","url":"pages/topics-in-applied-computation-advanced-practical-data-science/"},{"title":"Lecture 1: Notebook 1","text":"Lecture 4: Dask Dask is a useful tools 1. Add general description. Dask is a useful tools 2. Add infrastructure/platform description. Link to documentation Dask and why it is cool Dask developer community on twitter . Dask is a useful tools 3. a) What you can do you, b) what other advanced things people do. We'll go over the basics of some Dask services, but we should point out that a lot of talented people have given tutorials, check'em out. BLA BLA Table of Contents Lecture 4: Dask Part 1: The building block of scalable computing Public cloud basics Cloud pros and cons Getting started: first access to Azure Create your Azure free account Login to Azure Dashboard and create your first service Create API key to use Microsoft Azure service Script: create a set of keys for using Azure services Recap What you have learnt What you will learn next guide ADD TO LECTURE PRESENTATION <1.1 Why Dask> <1.2 Cooking with DAGs> <1.3 Scaling out, concurrency, and autorecovery> <2.1 Hello Dask: A first look at the DataFrame API> <2.2 Visualizing DAG> <2.3 Task Scheduling> <3.1 Why to use DataFrames> <3.2 Dask and Pandas: DataFrame partitioning and shuffle> <3.3 Limitations> GO TO NOTEBOOK (MAYBE ONE SUMMARY SLIDE AND JUMP INTO NOTEBOOK) <4.1 Read data, datasets, and datatypes> <4.2 Reading from relational database, HDFS, and S3> <5.1 Indexes, selecting, and dropping> <5.1.1 Filtering and Reindexing> <5.2 Joining, concatenating, and unioning> <5.2.1 Recording data> <5.3 Elementwise operations> <5.4 Dealing with missing values> <6.1 Descriptive Statistics> <6.2 Built-In aggregate functions> <6.3 Custom aggregate functions> <6.4 (Rolling window) functions> <7.1 Preare-reduce-collect-plot> <7.2 Continuous and categorial> <7.3 Density plots> <7.4 Random samples> <7.5 Heatmap> SKIP NEXT OR THIS IS THE MOST INTERESTING? SHOW CASE NOTEBOOK IN CLASS <9.1 Reading and parsing unstructured data with Bags> <9.1.1 Selecting and viewing data from a Bag> <9.1.2 Common parsing issue> <9.1.3 Delimiters> <9.2 Transforming, filtering, and folding> <9.2.1 Map, filter, and aggregate functions (foldby)> <9.2.2 Building Arrays from bags> <9.2.3 Summary stats on bags: parallel text analysis> <9.2.3.1 Bigrams> <9.2.3.2 Tokens and filtering stopwords> <9.2.3.2 Analyze bigrams> <10.1 Linear Models with Dask-ML> <10.2 Evaluating and tuning Dask-ML models> <10.3 Persisting Dask-ML models> EXERCISE: YOU LEARNED IT, NOW DO IT. REPRODUCE A PAPER FROM READING LIST <11.1 Building a Dask cluster on Amazon AWS with Docker> <11.1.1 to 11.1.7> <11.2 Running and monitoring Dask jobs on a cluster> <11.3 Cleaning up the Dask clusters on AWS> Part 1: Scalable computing If you ask different people, you'll get different answers, but one of the commonalities is that most people don't realize is that eventhough these services come with costs (i.e. both monetary and training), they provides great resources that social scientists should start exploring themself. Here are some highlights: You can use them \"anytime, anywhere\": public cloud users can access, barely always, cloud services and keep their data stored safely in the infrastructure. You won't need to plan far ahead for provisioning: public cloud users can use infinite computing and storaging resources available on demand. In this way, the user can offload some problems to the service provider such as mantaining both hardware and software. You can buy what you need, when you need it: public cloud allows you to use services eliminating any sort of up-front commitment by Cloud user. Public cloud allows teams to collaborate: Public cloud allows you to share data and collaborate more easily. Public cloud basics Cloud Computing refers to both the applications delivered as services over the Internet and the hardware/systems software in the datacenters that provide those services. The datacenter hardware and software is what we will call a Cloud . When a Cloud is made available to the public (through pay-as-you-go services), it is called a Public Cloud; the service being sold from the datacenter to the provider such as Microsoft, Google, Amazon or IBM (which might or not might be the same) called computing utility [ 1 ] and the one sold from provider to user that we will refer as a web application or more in general a service. Current examples of public computing services include AmazonWeb Services, Google AppEngine, and Microsoft Azure. A differenet deployment system from the public is the private. The private Cloud refers to internal datacenters of a business or other organization that are not made available to the public. The figure below shows the roles of the people as users or providers of different layers of the Cloud. Image adapted from Armbust et al., 2009 The National Institute of Standards and Technology (NIST) defines cloud as [ 2 ]: \"a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.\" Cloud computing has 5 essential characteristics: Measured Service Rapid Elasticity Broad Network Access Resource Pooling On Demand Self Service You can think about cloud computing as composition of three different areas/models: The Infrastructure as a Service ( IaaS ) is a cloud of resources provided to users. IaaS provides the basic functionality of storage and computing as service consisting in network servers, storage systems, network instrumentality, and information headquarters. The Platform as a Service ( PaaS ) is a development environment provided as a service. These advanced levels of alternative service can be designed by end-users. PaaS offers virtualized servers that multiple users work on applications or grow innovative applications, with no having to concern about keeping operating systems, server hardware, equalization or weight power calculation. The Software as a Service ( SaaS ) is an application that is offered to consumers using the Internetwork. A single case of service works in the cloud and multiple end users services. An example of Cloud Computing is Software as a Service (SaaS), where you input data on software and the data is transformed remotely through a software interface without your computer being involved. Thus SaaS eliminates client fears on storage, server applications, application development, and related common concerns of IT. In this to tutorial we are going to show you examples encompasses PaaS, when you will build the experiment using the two tutorials, and SaaS, when you will learn how to use the cloud services in the three guides. Cloud pros and cons Overall, the advantages of using a cloud outperforms the disadvantage. When deciding to build your own application, which an example in this series of guides is the experiment, it is important to consider a couple of aspects: cloud security, and cloud service accuracy. In respect to the first, we reccomend making sure that data being stored in the cloud meet the requirements of your institution/research as well as checking the warranty of the chosen provider. In respect to the second, we suggest to consider the accuracy of the algorithm provided by the service for the purpose of the research but this fall outside the purpose of this workshop. Naeem et al.[ 3 ] discusses some of the advantages and disadvantages when using cloud computing. We report them from their study in the chart below: Disadvantages Advantages Security in the Cloud Almost Unlimited Storage Technical Issues Quick Development Non-Interoperability Cost Efficient Dependency and vendor lock-in Automatic Software Integration Internet Required Shared Resources Less Reliability Easy Access to Information Less management Mobility Raised Vulnerability Better Hardware Management Prone to Attack Backup and Recovery Getting started: first access to Azure Create your Azure free account To access Azure cloud computing services, you will have to sign up for an Azure free account, if you don't already have one. If you do not have a Microsoft account either you will be asked to create one, otherwise insert your outlook account (e.g. your_email_address@outlook.com). To create your Azure account you will be asked to add your credentials as well as a credit card account. This will not be charged unless you exceed the credit provided with the one month trial version [ 4 ]. We reccomend to cancel the account after the first month in case you are not interested in the service. Follow the next steps to set up a free account: Go to https://azure.microsoft.com/en-us/ and click on free account Click on start free Create an Azure account: choose name, set password, add security info, add credit/debit card information Login to Azure Dashboard and create your first service Once you have created your Azure free account, you just need to go to the Azure portal and login using the credentials. Go to https://portal.azure.com/ and sign-in to your account to access Azure Dashboard (note: familiarize with the relevant services?) You are now ready to deploy your first public cloud service! Follow the next steps: Click on create a resource Write on the bar the name of the service you want to subscribe to. For convinience we show the Storage Account that will we use in the next guide. Type storage account in the bar and then press enter You will be directed to the a view containing a short description of the service, as well as links to the documentation and pricing information. Click on create to start deploying the services. Next, complete entering the following intormation and click on create once finished: Account name: enter lowercase and globally unique name (e.g. \"mycloudstorageplayground\") Deployment model: click on Resource manager Account kind: Storage v1 Location: East US Replication: Locally-reduntant storage (LRS) Performance: click on Standard Secure transfer required: Enabled Subscription: Free Trial Select server region: Eastus Resource Group: create a new entering name (e.g. myresourcegroup) Virtual networks: click on Enabled Pin to dashboard (optional): [x] Once the service is deployed, you will see on your Dashbord a white box with your storage account's name. Click on the box with your storage_account_name to access the storage account interface. The storage account interface shows a summary of the settings defined in the previous steps and other utilities. On the top right box you can see the region from which your service is deployied, the type of storage you have choosen as well as the type of contents you decided to store (i.e. Locally Reduntant Storage which stands for data you might use a lot and the server will know). You can also find the id of your subrscition below erased for privacy purposes. Create API key to use Microsoft Azure service We have shown you how to login to the Azure portal and how to create a Storage Account, now it is time to retrieve the key necessary to use it. We will use the key in the next guide to make requests to Microsoft Azure using Application Programming Interface (API). The API functions as an intermediary that allows two applications to talk to each other, in our case our software and Azure SaaS. The API key allows Azure to identifies your subscription account and to bill it (unless you switch your free account to a pay as you go account your account will not be billed). To retrieve your Storage Account key, start from going to the dashboard and clicking on the box with your storage_account_name . Then, click on on Access keys on the side bar. Copy storage account name and key1, clicking on the icon in the left, and paste them in the script below. Script: create a set of keys for using Azure services In the next guides, we are going to poke around with several Azure services. We reccomend you to create all the services on the list below and to save the name you will give to each service and primary key in the cell below. Here is a complete list of the services that you should create: Storage Account Face Computer Vision Bing Speech Recognition Text Analytics When looking for a service, we recommend to click on the Create a Resource button and to copy each service name on the finder bar as shown before. This will avoid you to look for service at the time, and some headache from navigating yourself through the myriad of services available. Run the cell when you are done, and a file with your key will be automatically generated and stored into the folder public_cloud_computing/guides/keys. In [ ]: ############################################################### # copy and paste your services' account name and primary key # ############################################################### # STORAGE_ACCOUNT STORAGE_ACCOUNT_NAME = '' #add your account name STORAGE_ACCOUNT_API_KEY = '' #add your account key1 # COGNITIVE_SCIENCE_FACE_ACCOUNT FACE_ACCOUNT_NAME = '' FACE_API_KEY = '' # COGNITIVE_SCIENCE_COMPUTER_VISION_ACCOUNT COMPUTER_VISION_NAME = '' COMPUTER_VISION_API_KEY = '' # SPEECH_RECOGNITION_ACCOUNT SPEECH_RECOGNITION_NAME = '' SPEECH_RECOGNITION_KEY = '' # TEXT_ANALYTICS_ACCOUNT TEXT_ANALYTICS_NAME = '' TEXT_ANALYTICS_API_KEY = '' #run this cell to write a copy of your Azure services information (NAME and API's key) #write a dictionary azure_services_keys = { 'STORAGE' : { 'NAME' : STORAGE_ACCOUNT_NAME , 'API_KEY' : STORAGE_ACCOUNT_API_KEY }, 'FACE' : { 'NAME' : FACE_ACCOUNT_NAME , 'API_KEY' : FACE_API_KEY }, 'COMPUTER_VISION' : { 'NAME' : COMPUTER_VISION_NAME , 'API_KEY' : COMPUTER_VISION_API_KEY }, 'SPEECH_RECOGNITION' : { 'NAME' : SPEECH_RECOGNITION_NAME , 'API_KEY' : SPEECH_RECOGNITION_KEY }, 'TEXT_ANALYTICS' : { 'NAME' : TEXT_ANALYTICS_NAME , 'API_KEY' : TEXT_ANALYTICS_API_KEY }} #dump the dictionary on a file and saved in the folder < /guides/keys > #import modules import pickle import json #open a .json file and copy the dictionary with all your keys with open ( \"../keys/azure_services_keys.json\" , 'wb' ) as f : pickle . dump ( azure_services_keys , f ) ################################ # run this cell once completed # ################################ Recap What you have learnt What is cloud and its advantages Access the Azure portal How to deploy public cloud service Now that you know more about cloud, what do you think about it? What you will learn next guide How to use public cloud services: What is a cloud storage Access Azure cloud storage using Storage Explorer UI and with Python SDK Create BLOB container and Upload BLOB (Big Large Binary Objects AKA image, audio, etc.) Question for you Now that you know more about cloud, what do you think about it? When would it be useful in your work, research? Footnotes [1] Armbrust et al, 2009. Above the Clouds: A Berkeley View of Cloud Computing [2] Peter Mell and Timothy Grance, 2011. The NIST Definition of Cloud Computing: recommendations of the National Institute of Standards and Technology [3] Naeem et al, 2016. Cluster Computing vs Cloud Computing: a comparison and overview [4] At subscription of a free account you will receive 200 dollars for 30 days to try pay as you go cloud services and a free account for a year. Once you exceed 200 dollars or the 30 days free trial will expired you will be asked to upgrade your subscription. In [ ]: #import library to display notebook as HTML import os from IPython.core.display import HTML #path to .ccs style script cur_path = os . path . dirname ( os . path . abspath ( \"__file__\" )) new_path = os . path . relpath ( '.. \\\\ .. \\\\ styles \\\\ custom_styles_public_cloud_computing.css' , cur_path ) #function to display notebook def css (): style = open ( new_path , \"r\" ) . read () return HTML ( style ) In [ ]: #run this cell to apply HTML style css ()","tags":"Lectures","url":"lectures/lecture1/notebook/"},{"title":"Lecture 2: Notebook 2","text":"In [1]: import numpy as np In [2]: a = [ 1 , 2 , 3 , 5 ] summation = np . sum ( a ) print ( summation ) 11","tags":"Lectures","url":"lectures/lecture2/notebook/"},{"title":"Lecture 4: Dask","text":"AC295: Advanced Practical Data Science Lecture 4: Dask Harvard University Spring 2020 Instructors : Pavlos Protopapas TF : Michael Emanuel, Andrea Porelli and Giulia Zerbini Author : Andrea Porelli and Pavlos Protopapas Table of Contents Lecture 4: Dask Part 1: Scalable computing 1.1 Dask API 1.2 Directed acyclical graph (DAGs) ) 1.3 Review Part 1 Part 2: Introduction to DASK 2.1 Exploratory data analysis with DASK 2.2 Visualize directed acyclic graphs (DAGs) ) 2.3 Task scheduling 2.4 Review Part 2 Part 3: Exercise with DASK 3.1 Learn how to Manipulate Structured Data 3.2 Summary part 3 and Dask limitations Part 1: Scalable computing 1.1 Dask API what makes it unique: allows to work with larger datasets making it possible to parallelize computation it is helpful when size increases and even when \"simple\" sorting and aggregating would otherwise spill on persistent memory it simplifies the cost of using more complex infrastructure it is easy to learn for data scientists with a background in the Python (similar syntax) and flexible it helps applying distributed computing to data science project: not of great help for small size datasets: complex operations can be done without spilling to disk and slowing down process. Actually it would generate greater overheads. very useful for medium size dataset; it allows to work with medium size in local machine. Python was not designed to make sharing work between processes on multicore systems particularly easy. As a result, it can be difficult to take advantage of parallelism within Pandas. essential for large datasets: Pandas, NumPy, and scikit-learn are not suitable at all for datasets of this size, because they were not inherently built to operate on distributed datasets. This provide help for using libraries Dataset type Size range Fits in RAM? Fits on local disk? Small dataset Less than 2–4 GB Yes Yes Medium dataset Less than 2 TB No Yes Large dataset Greater than 2 TB No No Adapted from Data Science with Python and Dask Dask consists of several different components and APIs, which can be categorized into three layers: the scheduler: add_definition/explanation; low-level APIs: add_definition/explanation; and high-level APIs: add_definition/explanation . Adapted from Data Science with Python and Dask 1.2 Directed acyclical graph (DAGs) graph : is a representation of a set of objects that have a relationship with one another >>> good to represent a wide variety of information. A graph is compounded by: - *node*: a function, an object or an action - *line*: symbolizes the relationship among nodes directed acyclical graph : there is one logical way to traverse the graph. No node is visited twice. cyclical graph : exists a feedback loop that allow to revisit and repeat the actions within the same node. handle computational resources : as the problem we solve requires more resources we have two options: -*scale up*: increase size of the available resource. invest in more efficient technology, cons diminishing returns -*scale out*: add other resources (dask main idea). invest in more cheap resources, cons distribute workload concurrency : as we approach greater number of \"work to be completed\", some resources might be not fully exploited. For instance some might be idling because of insufficient shared resources (i.e. resource starvation) . Schedulers handle this issue making sure to provide sufficient resources to each worker. failures : - work failures: a worker leave, and you know have to assign another one to his task. This might potentially slowing down the execution, however it won't affect previous work (aka data loss) - data loss: some accident happens and you have to start from the beginning. The scheduler stop and restart from the beginning the whole process. 1.3 Review Part 1 Dask can be used to scale popular Python libraries such as Pandas and NumPy allowing to analyse dataset with greater size (>8GB) Dask uses directed acyclical graph to coordinate execution of parallelized code across processors Directed acyclical graph are made up of nodes, clearly defined start and end that can be transverse in only one logical way (no looping). Upstream actions are completed before downstream nodes. Scaling out (i.e. add workers) can improve performances of complex workloads, however create overhead that can reduces gains. In case of failure, the step to reach a node can be repeated from the beginning without disturbing the rest of the process. Part 2: Introduction to DASK Warm up with short example of data cleaning using Dask DataFrames Visualize Directed Acyclical Graph generated by Dask workloads with graphviz Explore how the scheduler applies the DAGs to coordinate execution code You will learn: Dask DataFrame API Use diagnostic tool Use low-level Delayed API to create custom graph 2.1 Exploratory data analysis with DASK Set up environment and working directory Load data Check data quality issue (e.g. missing values and outliers) Drop columns (not useful for analysis AKA missing many values) 2.1.1 Set up environment and working directory In [31]: # import libraries import sys import os ## import dask libraries import dask.dataframe as dd from dask.diagnostics import ProgressBar # import libraries import pandas as pd In [4]: # assign working directory [CHANGE THIS]. It can not fit in github so I have it locally. Download files os . chdir ( '/Users/pavlos/Teaching/2020/AC295_data/nyc-parking-tickets' ) cwd = os . getcwd () # print print ( ' ' , sys . executable ) print ( ' ' , cwd ) /Users/pavlos/anaconda3/bin/python /Users/pavlos/Teaching/2020/AC295_data/nyc-parking-tickets 2.1.2 Load data In [9]: ## read data using DataFrame API df = dd . read_csv ( 'Parking_Violations_Issued_-_Fiscal_Year_2017.csv' ) df Out[9]: Dask DataFrame Structure: Summons Number Plate ID Registration State Plate Type Issue Date Violation Code Vehicle Body Type Vehicle Make Issuing Agency Street Code1 Street Code2 Street Code3 Vehicle Expiration Date Violation Location Violation Precinct Issuer Precinct Issuer Code Issuer Command Issuer Squad Violation Time Time First Observed Violation County Violation In Front Of Or Opposite House Number Street Name Intersecting Street Date First Observed Law Section Sub Division Violation Legal Code Days Parking In Effect From Hours In Effect To Hours In Effect Vehicle Color Unregistered Vehicle? Vehicle Year Meter Number Feet From Curb Violation Post Code Violation Description No Standing or Stopping Violation Hydrant Violation Double Parking Violation npartitions=33 int64 object object object object int64 object object object int64 int64 int64 int64 float64 int64 int64 int64 object object object object object object object object object int64 int64 object object object object object object float64 int64 object int64 object object float64 float64 float64 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Dask Name: from-delayed, 99 tasks note that metadata are shown in the frame instead of data sample syntax is pretty similar to Pandas API # partitions is the number of splits used to separate the main dataset. The optimal number is decided by the scheduler that split Pandas DataFrame into smaller chuncks. In this case each partitions is ~64MB (i.e. Dataset size/npartitions = 2GB/33). If we have one worker, Dask will cycle to each partition one at time. data types are reported under each column name (similary to describe method in Pandas, however performed through random sampling because data scattered across multiple physical machines). Good practice is to explicit data types instead relying on Dask's inference (store in binary format ideally, see links to Write and Read Parquet). Dask Name reports the name of the DAG (i.e. from-delayed) # tasks is the number of nodes in the DAG. You can think of a task as a Python function and in this case each partition operates 3 tasks: 1) reading the raw data, 2) splitting the data in appropriate blocks and, 3) initialazing the DataFrame object. Every task comes with some overhead (between 200us and 1ms). Partition process from Data Science with Python and Dask Want to know more? Best Practices Write and Read Parquet 2.1.3 Check data quality issue In [12]: # count missing values missing_values = df . isnull () . sum () missing_values Out[12]: Dask Series Structure: npartitions=1 Date First Observed int64 Violation Time ... dtype: int64 Dask Name: dataframe-sum-agg, 166 tasks note that dask object created is a series containing metadata and syntax is pretty similar to Pandas API processing hasn't be completed yet, instead Dask prepared a DAG stored in the missing values variable (advantage of building graph quickly without need to wait for computation) # tasks increased because have been added 2 tasks (i.e. check missing values and sum) for each of the 33 partitions as well as a final addition to aggregate the results among all the partitions for a total of 166 = [(66+1)+99] In [14]: # calculate percent missing values mysize = df . index . size missing_count = (( missing_values / mysize ) * 100 ) missing_count Out[14]: Dask Series Structure: npartitions=1 Date First Observed float64 Violation Time ... dtype: float64 Dask Name: mul, 235 tasks note that dask object created is a series and computation hasn't be completed yet df.index.size is a dask object dask.dataframe.core.Scalar . You cannot access its value/lenght directely like you would do with a list (e.g. len()). It would go against the whole idea of dask (i.e. read all the dataset) # tasks increased because have been added 2 tasks (i.e. division and multiplication) data type ahs changed from int64 to float64. Dask automatically converted it once the datatype at the output might not match the input after the division In [15]: # run computations using compute method with ProgressBar (): missing_count_percent = missing_count . compute () missing_count_percent [##################################### ] | 92% Completed | 1min 6.8s /Users/pavlos/anaconda3/lib/python3.6/site-packages/dask/core.py:118: DtypeWarning: Columns (18,38) have mixed types. Specify dtype option on import or set low_memory=False. args2 = [_execute_task(a, cache) for a in args] [########################################] | 100% Completed | 1min 8.8s Out[15]: Summons Number 0.000000 Plate ID 0.006739 Registration State 0.000000 Plate Type 0.000000 Issue Date 0.000000 Violation Code 0.000000 Vehicle Body Type 0.395361 Vehicle Make 0.676199 Issuing Agency 0.000000 Street Code1 0.000000 Street Code2 0.000000 Street Code3 0.000000 Vehicle Expiration Date 0.000000 Violation Location 19.183510 Violation Precinct 0.000000 Issuer Precinct 0.000000 Issuer Code 0.000000 Issuer Command 19.093212 Issuer Squad 19.101506 Violation Time 0.000583 Time First Observed 92.217488 Violation County 0.366073 Violation In Front Of Or Opposite 20.005826 House Number 21.184968 Street Name 0.037110 Intersecting Street 68.827675 Date First Observed 0.000000 Law Section 0.000000 Sub Division 0.007155 Violation Legal Code 80.906214 Days Parking In Effect 25.107923 From Hours In Effect 50.457575 To Hours In Effect 50.457548 Vehicle Color 1.410179 Unregistered Vehicle? 89.562223 Vehicle Year 0.000000 Meter Number 83.472476 Feet From Curb 0.000000 Violation Post Code 29.530489 Violation Description 10.436611 No Standing or Stopping Violation 100.000000 Hydrant Violation 100.000000 Double Parking Violation 100.000000 dtype: float64 note that .compute() method is necessary to run the actions embedded in each node of the DAG The results of the compute method are stored into a Pandas Series ProgressBar() is a wrapper to keep track of running tasks. It shows completed work from a quick visual inspection we can see that are columns that are incomplete and we should drop 2.1.4 Drop columns In [16]: # filter sparse columns(greater than 60% missing values) and store them columns_to_drop = missing_count_percent [ missing_count_percent > 60 ] . index print ( columns_to_drop ) # drop sparse columns with ProgressBar (): #df_dropped = df.drop(columns_to_drop, axis=1).persist() df_dropped = df . drop ( columns_to_drop , axis = 1 ) . compute () Index(['Time First Observed', 'Intersecting Street', 'Violation Legal Code', 'Unregistered Vehicle?', 'Meter Number', 'No Standing or Stopping Violation', 'Hydrant Violation', 'Double Parking Violation'], dtype='object') [##################################### ] | 93% Completed | 1min 0.4s /Users/pavlos/anaconda3/lib/python3.6/site-packages/dask/core.py:118: DtypeWarning: Columns (18,38) have mixed types. Specify dtype option on import or set low_memory=False. args2 = [_execute_task(a, cache) for a in args] [########################################] | 100% Completed | 1min 1.9s note that use Pandas Series to drop columns in Dask DataFrames because each partition is a Pandas DataFrame In the case the Series is made available to all threads, in a cluster it would be serialized and brodcast to all the worker nodes .persit() method allows to store in memory intermediate computations so they can be reused. 2.2 Visualize directed acyclic graphs (DAGs) DASK uses graphviz library to generate visual representation of the DAGs created by the scheduler Use .visualize() metod to inspect the DAGs of DataFrames, Series, Bag, and arrays For simplicity we will use Dask Delayed object instead of DataFrames since they grow quite large and hard to visualize delayed is a constructor that allows to wrap functions and create Dask Delayed objects that are equivalent to a node in a DAG. By chaining together delayed object, you create the DAG Below are two examples, in the first one you build a DAG with one node only with dependencies and in the second one you build another with multiple nodes with dependencies 2.2.1 Example 1: DAG with one node with depenndecy In [17]: # import library import dask.delayed as delayed In [21]: def increment ( i ): return i + 1 def add ( x , y ): return x + y # wrap functions within dealyed object and chain x = delayed ( increment )( 1 ) y = delayed ( increment )( 2 ) z = delayed ( add )( x , y ) # visualize the DAG z . visualize () Out[21]: In [19]: # show the result z . compute () Out[19]: 5 note that to build a node wrap the function with the delayed object and then pass the arguments of the function. You could also use a decoretors (see documentation). circles symbolize function and computations while squares intermediate or final result incoming arrows represent dependecies. The increment function do not have any dependency while add function has two. Thus, add function has to wait until objects x and y have been calculated functions without dependencies can be computed independentely and a worker can be assigned to each one of them use method .visualize() on the last node with dependencies to peak at the DAG Dask does not compute the DAG . Use the method .compute() on the last node to see the result 2.2.2 Example 1: DAG with more than one node with dependencies we are going to build a more complex DAG with two layers: layer1 is built by looping over a list of data using a list comprehension to create dask delayed objects as \"leaves\" node. This layer combine the previously created function increment with the values in the list, then use the built in function sum to combine the results; layer2 is built looping on each object created in layer1 and In [26]: data = [ 1 , 2 , 3 , 4 , 5 ] # compile first layer and visualize layer1 = [ delayed ( increment )( i ) for i in data ] total1 = delayed ( sum )( layer1 ) total1 . visualize () Out[26]: In [27]: def double ( x ): return x * 2 # compile second layer and visualize layer2 = [ delayed ( double )( j ) for j in layer1 ] total2 = delayed ( sum )( layer2 ) #.persist() total2 . visualize () Out[27]: In [28]: z = total2 . compute () z Out[28]: 40 note that built in function persistent using .persist() and it will be represented as a rectangle in the graph In [29]: ## TASK # visualize DAGs built from the DataFrame missing_count . visualize () Out[29]: Want to know more? Parallelize non DataFrame code 2.3 Task scheduling Dask performs the so called lazy computations. Remember, until you run the method .compute() all what Dask does is to split the process into smaller logical pieces (which avoid loading all the dataset) Even though the process is defined, the number of resources assigned and the place where the results will be stored are note assigned because the scheduler assign them dynamically. This allow to recover from worker failure, network unreliability as well as workers completing the tasks at different speeds. Dask uses a central scheduler to orchestrate all this work. It splits the workload among different servers which unlikely they are perfectly assigned the same load, power or access to data. Due to these conditions, scheduler needs to promptly react to avoid bottlenecks that will affect overall runtime. For best performance, a Dask cluster should use a distributed file system (S3, HDFS) to back its data storage. Assuming there are two nodes like in the image below and data are stored in one. In order to perform computation in the other node we have to move the data from one to the other creating an overhead proportional to the size of the data. The remedy is to split data minimizing the number of data to broadcast across different local machines. Local disk from Data Science with Python and Dask Dask scheduler takes data locality into consideration when deciding where to perform computation. 2.4 Review Part 2 The beauty of this code is that you can reuse in one machine or a thousand Similar syntax helps transition from Pandas to Dask (good in general for refactoring your code) Dask DataFrames is a tool to parallelize computing done popular Pandas that allow to clean and ananlyze large dataset Dask parallize Pandas Dataframe and more in general work using DAGs Computation are structured by the task scheduler using DAGs Computation are constructed lazily and then compute the method Use visualize method for a visual representation Computation can persist in memory avoid slowdown to replicate result Data locality help minimizing network and IO latency Part 3: Exercise with DASK 3.1 Learn how to manipulate structured data Build a Dask DataFrame from a Pandas DataFrame In [32]: # create lists with actions action_IDs = [ i for i in range ( 0 , 10 )] action_description = [ 'Clean dish' , 'Dress up' , 'Wash clothes' , 'Take shower' , 'Groceries' , 'Take shower' , 'Dress up' , 'Gym' , 'Take shower' , 'Movie' ] action_date = [ '2020-1-16' , '2020-1-16' , '2020-1-16' , '2020-1-16' , '2020-1-16' , '2020-1-17' , '2020-1-17' , '2020-1-17' , '2020-1-17' , '2020-1-17' ] # store list into Pandas DataFrame action_pandas_df = pd . DataFrame ({ 'Action ID' : action_IDs , 'Action Description' : action_description , 'Date' : action_date }, columns = [ 'Action ID' , 'Action Description' , 'Date' ]) action_pandas_df Out[32]: Action ID Action Description Date 0 0 Clean dish 2020-1-16 1 1 Dress up 2020-1-16 2 2 Wash clothes 2020-1-16 3 3 Take shower 2020-1-16 4 4 Groceries 2020-1-16 5 5 Take shower 2020-1-17 6 6 Dress up 2020-1-17 7 7 Gym 2020-1-17 8 8 Take shower 2020-1-17 9 9 Movie 2020-1-17 In [33]: # convert Pandas DataFrame to a Dask DataFrame action_dask_df = dd . from_pandas ( action_pandas_df , npartitions = 3 ) In [34]: action_dask_df Out[34]: Dask DataFrame Structure: Action ID Action Description Date npartitions=3 0 int64 object object 4 ... ... ... 8 ... ... ... 9 ... ... ... Dask Name: from_pandas, 3 tasks In [38]: # info Dask Dataframe print ( ' ' , action_dask_df . divisions ) print ( '<# partitions>' , action_dask_df . npartitions ) (0, 4, 8, 9) <# partitions> 3 In [39]: # count rows per partition action_dask_df . map_partitions ( len ) . compute () Out[39]: 0 4 1 4 2 2 dtype: int64 In [43]: # filter entries in dask dataframe print ( ' \\n ' ) action_filtered = action_dask_df [ action_dask_df [ 'Action Description' ] != 'Take shower' ] print ( action_filtered . map_partitions ( len ) . compute ()) print ( ' \\n ' ) action_filtered_reduced = action_filtered . repartition ( npartitions = 1 ) print ( action_filtered_reduced . map_partitions ( len ) . compute ()) 0 3 1 3 2 1 dtype: int64 0 7 dtype: int64 3.2 Summary part 3 and dask limitations dataframe are immutable. Functions such as pop and insert are not supported does not allow for functions with a lot of shuffeling like stack/unstack and melt limit this operations after major filter and preprocessing join, merge, groupby, and rolling are supported but expensive due to shuffeling reset index starts sequential counting for each partitions apply and iterrow are known to be inefficient in Pandas, the same for Dask use .division() to inspect how DataFrame has been partitioned for best performance partitions should be rougly equal. Use .repartition() method to balance across datasets for best performance sort by logical columns, partition by index and index should be presorted","tags":"Lectures","url":"lectures/lecture4/notebook/"}]}